# Task Sentiment Analyzer

## 📖 Project Overview
**Task Sentiment Analyzer** is a **Machine Learning and NLP-based project** that performs **sentiment analysis** on text data. The goal is to classify comments as **Positive** or **Negative**. The project explores multiple algorithms, including **traditional machine learning**, **ensemble models**, and **deep learning**, using the **IMDb Movie Reviews dataset** for training.  

The final deployed model uses **Logistic Regression** for simplicity, robustness, and high accuracy, accessible via an **interactive Streamlit web app**.

---

## 🧩 Dataset
The project uses the **IMDb Movie Reviews Dataset**:  
- **Source:** [Kaggle](https://www.kaggle.com/datasets/mantri7/imdb-movie-reviews-dataset)  
- **Initial file:** `train_data (1).csv`  
- **Cleaned dataset:** `reddit_preprocessing.csv`  

---

## 🧹 Data Preprocessing & EDA
All work started in **Jupyter Notebooks**. The workflow was:

1. **Load raw dataset** in the first notebook:  
```python
df = pd.read_csv('train_data (1).csv')

2. **save clean data**

df.to_csv('reddit_preprocessing.csv', index=False)

## ⏱️ Timeline & Workflow Diagram

```mermaid
flowchart TD
    A[Download Dataset from Kaggle] --> B[Load raw data in Jupyter Notebook]
    B --> C[Data Preprocessing]
    C --> D[Save Cleaned Dataset as reddit_preprocessing.csv]
    D --> E[Exploratory Data Analysis (EDA)]
    E --> F[Model Experiments]
    F --> F1[Bag-of-Words & TF-IDF (Logistic Regression)]
    F --> F2[Naive Bayes with HPT]
    F --> F3[XGBoost]
    F --> F4[LightGBM]
    F --> F5[Random Forest]
    F --> F6[LSTM Deep Learning]
    F --> F7[Stacking Classifier]
    F1 --> G[Select Best Model: Logistic Regression]
    F7 --> G
    G --> H[Save Models: logreg_model.pkl & vectorizer_model.pkl]
    H --> I[Deploy using Streamlit]
    I --> J[Real-time Sentiment Analysis App]


### How it works:
1. **Dataset** is downloaded from Kaggle.  
2. **Jupyter Notebook** is used to load and inspect raw data.  
3. **Data Preprocessing**: HTML removal, lowercase conversion, stopword removal, etc.  
4. **Cleaned data** is saved as `reddit_preprocessing.csv`.  
5. **EDA** provides understanding of data distributions and patterns.  
6. **Model Experiments** test multiple algorithms (Logistic Regression, Naive Bayes, XGBoost, LightGBM, Random Forest, LSTM, Stacking).  
7. **Model Selection** chooses Logistic Regression for deployment.  
8. **Models are saved**, and **Streamlit app** is deployed for real-time predictions.

---

✅ This diagram gives a **clear high-level view** of your workflow from dataset download to final deployment.

---

If you want, I can **integrate this directly into your previous README** with all sections combined, so you’ll have a **complete polished document** ready to submit or push to GitHub.  

Do you want me to do that?